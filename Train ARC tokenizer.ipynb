{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new tokenizer for ARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer for google/long-t5-tglobal-base\n"
     ]
    }
   ],
   "source": [
    "model_to_fine_tune = \"google/long-t5-tglobal-base\"\n",
    "training_data_file = 'train_74444_with_letters_none_noise.csv'\n",
    "from transformers import AutoTokenizer\n",
    "print(f\"loading tokenizer for {model_to_fine_tune}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: This is a test\n",
      "tokenized text: ['▁This', '▁is', '▁', 'a', '▁test']\n",
      "original arc text: train input1 077 777 077 output1 89 9 9 07 000077077 000777777 000077077 077077077 777777777 077077077 000077077 000777777 000077077. input2 404 000 040 output2 89 9 9 40 404000404 000000000 040000040 000000000 000000000 000000000 000404000 000000000 000040000. input3 000 002 202 output3 89 9 9 02 000000000 000000000 000000000 000000000 000000002 000000202 000000000 002000002 202000202. input4 660 600 066 output4 89 9 9 60 660660000 600600000 066066000 660000000 600000000 066000000 000660660 000600600 000066066. input5 222 000 022 output5 89 9 9 20 222222222 000000000 022022022 000000000 000000000 000000000 000222222 000000000 000022022. test tinput1 707 707 770 toutput1 \n",
      "tokenized arc text: ['▁train', '▁input', '1', '▁07', '7', '▁', '777', '▁07', '7', '▁output', '1', '▁', '89', '▁9', '▁9', '▁07', '▁', '0000', '770', '77', '▁000', '777', '777', '▁', '0000', '770', '77', '▁07', '70', '770', '77', '▁', '777', '777', '777', '▁07', '70', '770', '77', '▁', '0000', '770', '77', '▁000', '777', '777', '▁', '0000', '770', '77', '.', '▁input', '2', '▁', '404', '▁000', '▁04', '0', '▁output', '2', '▁', '89', '▁9', '▁9', '▁40', '▁', '404', '000', '404', '▁000', '000', '000', '▁04', '00', '000', '40', '▁000', '000', '000', '▁000', '000', '000', '▁000', '000', '000', '▁000', '404', '000', '▁000', '000', '000', '▁000', '04', '0000', '.', '▁input', '3', '▁000', '▁', '00', '2', '▁', '202', '▁output', '3', '▁', '89', '▁9', '▁9', '▁02', '▁000', '000', '000', '▁000', '000', '000', '▁000', '000', '000', '▁000', '000', '000', '▁000', '0000', '02', '▁000', '000', '202', '▁000', '000', '000', '▁', '00', '200', '000', '2', '▁20', '2000', '202', '.', '▁input', '4', '▁', '660', '▁600', '▁06', '6', '▁output', '4', '▁', '89', '▁9', '▁9', '▁60', '▁', '660', '66', '0000', '▁600', '600', '000', '▁06', '60', '66', '000', '▁6', '6000', '0000', '▁600', '000', '000', '▁06', '6000', '000', '▁000', '660', '660', '▁000', '600', '600', '▁', '0000', '660', '66', '.', '▁input', '5', '▁', '222', '▁000', '▁02', '2', '▁output', '5', '▁', '89', '▁9', '▁9', '▁20', '▁', '222', '222', '222', '▁000', '000', '000', '▁02', '202', '20', '22', '▁000', '000', '000', '▁000', '000', '000', '▁000', '000', '000', '▁000', '222', '222', '▁000', '000', '000', '▁000', '02', '202', '2.', '▁test', '▁', 't', 'in', 'put', '1', '▁7', '07', '▁7', '07', '▁', '770', '▁tout', 'put', '1']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test\"\n",
    "print(f\"original text: {text}\")\n",
    "print(f\"tokenized text: {tokenizer.tokenize(text)}\")\n",
    "\n",
    "# arc text\n",
    "arc_text = \"train input1 077 777 077 output1 89 9 9 07 000077077 000777777 000077077 077077077 777777777 077077077 000077077 000777777 000077077. input2 404 000 040 output2 89 9 9 40 404000404 000000000 040000040 000000000 000000000 000000000 000404000 000000000 000040000. input3 000 002 202 output3 89 9 9 02 000000000 000000000 000000000 000000000 000000002 000000202 000000000 002000002 202000202. input4 660 600 066 output4 89 9 9 60 660660000 600600000 066066000 660000000 600000000 066000000 000660660 000600600 000066066. input5 222 000 022 output5 89 9 9 20 222222222 000000000 022022022 000000000 000000000 000000000 000222222 000000000 000022022. test tinput1 707 707 770 toutput1 \"\n",
    "print(f\"original arc text: {arc_text}\")\n",
    "print(f\"tokenized arc text: {tokenizer.tokenize(arc_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_training_data(file):\n",
    "    # read the csv file\n",
    "    df = pd.read_csv(file)\n",
    "    for start_idx in range(0, len(df), 1000):\n",
    "        # combine prompt and correct_answer columns\n",
    "        df[\"prompt\"] = df[\"prompt\"] + df[\"correct_answer\"]\n",
    "        samples = df[start_idx : start_idx + 1000]\n",
    "        yield samples[\"prompt\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arc_tokenizer\\\\tokenizer_config.json',\n",
       " 'arc_tokenizer\\\\special_tokens_map.json',\n",
       " 'arc_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus = get_training_data(training_data_file)\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, tokenizer.vocab_size)\n",
    "new_tokenizer.save_pretrained(\"arc_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original arc text: train input1 077 777 077 output1 89 9 9 07 000077077 000777777 000077077 077077077 777777777 077077077 000077077 000777777 000077077. input2 404 000 040 output2 89 9 9 40 404000404 000000000 040000040 000000000 000000000 000000000 000404000 000000000 000040000. input3 000 002 202 output3 89 9 9 02 000000000 000000000 000000000 000000000 000000002 000000202 000000000 002000002 202000202. input4 660 600 066 output4 89 9 9 60 660660000 600600000 066066000 660000000 600000000 066000000 000660660 000600600 000066066. input5 222 000 022 output5 89 9 9 20 222222222 000000000 022022022 000000000 000000000 000000000 000222222 000000000 000022022. test tinput1 707 707 770 toutput1 \n",
      "length of tokenized arc text: 209\n",
      "tokenized arc text: ['▁', 't', 'r', 'a', 'i', 'n', '▁', 'i', 'n', 'p', 'u', 't', '1', '▁077', '▁777', '▁077', '▁', 'o', 'u', 't', 'p', 'u', 't', '1', '▁89', '▁9', '▁9', '▁07', '▁000077077', '▁000777777', '▁000077077', '▁', '077077077', '▁777777777', '▁', '077077077', '▁000077077', '▁000777777', '▁000077077', '.', '▁', 'i', 'n', 'p', 'u', 't', '2', '▁404', '▁000', '▁04', '0', '▁', 'o', 'u', 't', 'p', 'u', 't', '2', '▁89', '▁9', '▁9', '▁4', '0', '▁404000404', '▁000000000', '▁040000040', '▁000000000', '▁000000000', '▁000000000', '▁000404000', '▁000000000', '▁000040000', '.', '▁', 'i', 'n', 'p', 'u', 't', '3', '▁000', '▁002', '▁202', '▁', 'o', 'u', 't', 'p', 'u', 't', '3', '▁89', '▁9', '▁9', '▁02', '▁000000000', '▁000000000', '▁000000000', '▁000000000', '▁00000000', '2', '▁000000202', '▁000000000', '▁002000002', '▁', '202000202', '.', '▁', 'i', 'n', 'p', 'u', 't', '4', '▁660', '▁6', '0', '0', '▁066', '▁', 'o', 'u', 't', 'p', 'u', 't', '4', '▁89', '▁9', '▁9', '▁6', '0', '▁66066', '0000', '▁60060', '0000', '▁066066', '000', '▁66', '0000000', '▁6', '00000000', '▁066', '000000', '▁000660660', '▁000600600', '▁0000', '66066', '.', '▁', 'i', 'n', 'p', 'u', 't', '5', '▁222', '▁000', '▁022', '▁', 'o', 'u', 't', 'p', 'u', 't', '5', '▁89', '▁9', '▁9', '▁20', '▁222222222', '▁000000000', '▁', '022022022', '▁000000000', '▁000000000', '▁000000000', '▁000222222', '▁000000000', '▁000022022', '.', '▁', 't', 'e', 's', 't', '▁', 't', 'i', 'n', 'p', 'u', 't', '1', '▁707', '▁707', '▁77', '0', '▁', 't', 'o', 'u', 't', 'p', 'u', 't', '1']\n",
      "original arc text: train input1 077 777 077 output1 89 9 9 07 000077077 000777777 000077077 077077077 777777777 077077077 000077077 000777777 000077077. input2 404 000 040 output2 89 9 9 40 404000404 000000000 040000040 000000000 000000000 000000000 000404000 000000000 000040000. input3 000 002 202 output3 89 9 9 02 000000000 000000000 000000000 000000000 000000002 000000202 000000000 002000002 202000202. input4 660 600 066 output4 89 9 9 60 660660000 600600000 066066000 660000000 600000000 066000000 000660660 000600600 000066066. input5 222 000 022 output5 89 9 9 20 222222222 000000000 022022022 000000000 000000000 000000000 000222222 000000000 000022022. test tinput1 707 707 770 toutput1 \n",
      "tokenized arc text: 244\n"
     ]
    }
   ],
   "source": [
    "arc_text = \"train input1 077 777 077 output1 89 9 9 07 000077077 000777777 000077077 077077077 777777777 077077077 000077077 000777777 000077077. input2 404 000 040 output2 89 9 9 40 404000404 000000000 040000040 000000000 000000000 000000000 000404000 000000000 000040000. input3 000 002 202 output3 89 9 9 02 000000000 000000000 000000000 000000000 000000002 000000202 000000000 002000002 202000202. input4 660 600 066 output4 89 9 9 60 660660000 600600000 066066000 660000000 600000000 066000000 000660660 000600600 000066066. input5 222 000 022 output5 89 9 9 20 222222222 000000000 022022022 000000000 000000000 000000000 000222222 000000000 000022022. test tinput1 707 707 770 toutput1 \"\n",
    "print(f\"original arc text: {arc_text}\")\n",
    "tokenized_text= new_tokenizer.tokenize(arc_text)\n",
    "print(f\"length of tokenized arc text: {len(tokenized_text)}\")\n",
    "print(f\"tokenized arc text: {tokenized_text}\")\n",
    "# old tokenizer\n",
    "print(f\"original arc text: {arc_text}\")\n",
    "print(f\"tokenized arc text: {len(tokenizer.tokenize(arc_text))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c58e9361bde7ca617934da376e83056db506761bdc9593ca2087fabac973f609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
